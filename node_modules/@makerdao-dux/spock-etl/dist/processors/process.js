"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.findExtractorDependencies = exports.getAllDependencies = exports.getNextBlocks = exports.processBlocks = exports.process = void 0;
const node_1 = require("@sentry/node");
const lodash_1 = require("lodash");
const db_1 = require("../db/db");
const Job_1 = require("../db/models/Job");
const arrays_1 = require("../utils/arrays");
const breakout_1 = require("../utils/breakout");
const logger_1 = require("../utils/logger");
const promises_1 = require("../utils/promises");
const state_1 = require("./state");
const types_1 = require("./types");
const logger = logger_1.getLogger('extractor/index');
async function process(services, processors) {
    logger.debug('Spawning extractors: ', processors.length);
    while (processors.length > 0 && !breakout_1.getSpockBreakout()) {
        // NOTE: no two processors extract at the same
        let processed = 0;
        for (const p of processors) {
            const processedNow = await processBlocks(services, p);
            processed += processedNow;
        }
        // if we didn't process anything new introduce artificial delay before next run
        if (processed === 0) {
            await promises_1.delay(1000);
        }
    }
    logger.warn('Processing done');
}
exports.process = process;
async function processBlocks(services, processor) {
    const blocks = await getNextBlocks(services, processor);
    if (blocks.length === 0) {
        return 0;
    }
    // We can speed up whole process (process blocks in batches) if we don't have a risk of reorg.
    // Otherwise we process blocks separately to avoid problems with reorgs while processing tip of the blockchain.
    // By setting extractorWorker.reorgBuffer = 0 you can switch off this behaviour on chains that can't reorg (POA networks).
    const closeToTheTipOfBlockchain = services.config.extractorWorker.reorgBuffer === 0
        ? false
        : (lodash_1.get(blocks, '[0].number') || 0) +
            services.config.extractorWorker.batch -
            services.networkState.latestEthereumBlockOnStart +
            services.config.extractorWorker.reorgBuffer >
            0;
    const batchProcessing = !closeToTheTipOfBlockchain || processor.disablePerfBoost || false;
    const blocksInBatches = !batchProcessing ? blocks.map((b) => [b]) : arrays_1.findConsecutiveSubsets(blocks, 'number');
    logger.debug(`Processing ${blocks.length} blocks with ${processor.name}. Process in batch: ${batchProcessing}`);
    try {
        for (const blocks of blocksInBatches) {
            logger.trace(`Extracting blocks: ${blocks.map((b) => b.number).join(', ')}`);
            await services.db.tx(async (tx) => {
                const txServices = {
                    ...services,
                    tx,
                };
                if (types_1.isExtractor(processor)) {
                    await processor.extract(txServices, blocks);
                }
                else {
                    const realDeps = findExtractorDependencies(processor.dependencies, services.config.extractors);
                    const data = await Promise.all(realDeps.map((dep) => dep.getData(txServices, blocks)));
                    await processor.transform(txServices, data);
                }
                logger.debug(
                // prettier-ignore
                `Marking blocks as processed from ${blocks[0].number} to ${blocks[0].number + blocks.length} with ${processor.name}`);
                await markBlocksProcessed(tx, blocks, processor, services.processorSchema);
                logger.debug(`Closing db transaction for ${blocks[0].number} to ${blocks[0].number + blocks.length}`);
            });
        }
        state_1.clearProcessorState(services, processor);
    }
    catch (e) {
        logger.warn(
        // prettier-ignore
        `WARN[]: Error occured while processing: ${blocks[0].number} - ${blocks[0].number + blocks.length} with ${processor.name}`, e);
        state_1.addProcessorError(services, processor, e);
        if (state_1.getProcessorErrors(services, processor).length > services.config.processorsWorker.retriesOnErrors) {
            logger.error(`Stopping ${processor.name}. Restart ETL to continue`);
            const allErrors = JSON.stringify(state_1.getProcessorErrors(services, processor));
            logger.error('ERROR:', allErrors);
            node_1.captureException(e);
            await db_1.withConnection(services.db, async (c) => {
                await Job_1.stopJob(c, processor.name, allErrors, services.processorSchema);
            });
            state_1.clearProcessorState(services, processor);
        }
    }
    return blocks.length;
}
exports.processBlocks = processBlocks;
async function getNextBlocks(services, processor) {
    const { db, config, processorSchema: schema } = services;
    return db_1.withConnection(db, async (c) => {
        const batchSize = config.extractorWorker.batch;
        const job = await Job_1.getJob(c, processor.name, schema);
        if (!job) {
            throw new Error(`Missing processor: ${processor.name}`);
        }
        if (job.status !== 'processing') {
            logger.info(`Processors excluded from processing. Status is: ${job.status}`);
            return [];
        }
        const dependencies = getAllDependencies(processor);
        const query = dependencies.length === 0
            ? `
    SELECT b.* FROM ${schema}.block b
      WHERE b.id > ${job.last_block_id} ORDER BY id
      LIMIT ${batchSize};`
            : `
      SELECT b.* FROM ${schema}.block b
        WHERE id<= (SELECT MIN(last_block_id) FROM ${schema}.job  WHERE name in(
          ${dependencies.map((dependency) => `'${dependency}'`).join(',')}
        )) AND b.id > ${job.last_block_id} ORDER BY id
      LIMIT ${batchSize};`;
        const nextBlocks = (await c.manyOrNone(
        // prettier-ignore
        query)) || [];
        return lodash_1.sortBy(nextBlocks, 'id');
    });
}
exports.getNextBlocks = getNextBlocks;
async function markBlocksProcessed(connection, blocks, processor, schema) {
    const lastId = arrays_1.getLast(blocks).id;
    const updateJobSQL = `
  UPDATE ${schema}.job
  SET last_block_id = ${lastId}
  WHERE name='${processor.name}'
  `;
    await connection.none(updateJobSQL);
}
function getAllDependencies(p) {
    if (types_1.isExtractor(p)) {
        return p.extractorDependencies || [];
    }
    else {
        return [...(p.dependencies || []), ...(p.transformerDependencies || [])];
    }
}
exports.getAllDependencies = getAllDependencies;
function findExtractorDependencies(dependencies, allExtractors) {
    const extractorsByName = lodash_1.groupBy(allExtractors, 'name');
    const result = [];
    for (const d of dependencies) {
        const realDep = extractorsByName[d];
        if (!realDep || !realDep[0]) {
            throw new Error(`Dependency ${realDep} couldn't be found!`);
        }
        result.push(realDep[0]);
    }
    return result;
}
exports.findExtractorDependencies = findExtractorDependencies;
//# sourceMappingURL=process.js.map